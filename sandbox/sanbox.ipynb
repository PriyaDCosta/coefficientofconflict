{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the module you want to import\n",
    "tpm_directory = '/Users/priyadcosta/Documents/GitHub/coefficientofconflict/team-process-map/feature_engine'\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(tpm_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Basic Pre-processing\n",
    "\n",
    "Converting the labels to numbers and averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/priyadcosta/Documents/GitHub/coefficientofconflict/tpm-data-anotation/CONFLICT_CONVO_LABELING_LOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the labels into numeric scores\n",
    "\"\"\"\n",
    "\n",
    "def get_numeric_labels(text):\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Initialize the result variable\n",
    "    result = 0\n",
    "    \n",
    "    # Check if \"yes\" is present in the text\n",
    "    if 'yes' in text_lower:\n",
    "        result = 1\n",
    "    elif 'no' in text_lower:\n",
    "        result = 0\n",
    "    \n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "Convert all the columns to numeric labels\n",
    "\"\"\"\n",
    "def convert_labels(df):\n",
    "    \n",
    "    df['d_content'] = df['rating_directness_content'].apply(get_numeric_labels)\n",
    "    df['d_expression'] = df['rating_directness_expression'].apply(get_numeric_labels)\n",
    "    df['oi_content'] = df['rating_OI_content'].apply(get_numeric_labels)\n",
    "    df['oi_expression'] = df['rating_OI_expression'].apply(get_numeric_labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get the average of the ratings for a single column\n",
    "\"\"\"\n",
    "def get_averages(df,on_column):\n",
    "\n",
    "    # Calculate average ratings\n",
    "    average_ratings = df.groupby(['CONV_ID', 'id'])[on_column].mean().reset_index()\n",
    "\n",
    "    # Merge average ratings with original DataFrame\n",
    "    df = df.merge(average_ratings, on=['CONV_ID', 'id'], how='left', suffixes=('', '_average'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get the average ratings for all the columns\n",
    "\"\"\"\n",
    "def average_labels(df, columns):\n",
    "    for column in columns:\n",
    "        df = get_averages(df, column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determine the labels for the dataset\n",
    "\"\"\"\n",
    "def get_label(conv_id):\n",
    "    if conv_id.endswith('_A') or conv_id.endswith('_B'):\n",
    "        return 'winning'\n",
    "    else:\n",
    "        return 'awry'\n",
    "\n",
    "\"\"\" \n",
    "Get the dataset which the conversation belongs to awry or winning\n",
    "\"\"\"\n",
    "def dataset_labels(df):\n",
    "    df['dataset'] = df['CONV_ID'].apply(get_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop unncessary columns \n",
    "\"\"\"\n",
    "def drop_cols(df,type):\n",
    "    if type == 'average':\n",
    "        return df[['d_content_average', 'd_expression_average', 'oi_content_average','oi_expression_average', 'dataset']]\n",
    "    else:\n",
    "        return df[['d_content', 'd_expression', 'oi_content','oi_expression','dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset to which the chat belongs\n",
    "dataset_labels(data)\n",
    "\n",
    "#convert the text labels to numeric labels\n",
    "convert_labels(data)\n",
    "\n",
    "#get the average rating for each chat\n",
    "numeric_cols = ['d_content', 'd_expression', 'oi_content', 'oi_expression']\n",
    "data = average_labels(data,numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awry convos 32\n",
      "winning convos 26\n"
     ]
    }
   ],
   "source": [
    "print('awry convos ' + str(data[data['dataset'] == 'awry']['CONV_ID'].nunique()))\n",
    "print('winning convos ' + str(data[data['dataset'] == 'winning']['CONV_ID'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = drop_cols(data,'average')\n",
    "original_data = drop_cols(data,'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_logistic_regression(df,target_column):\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    y = df[target_column]   \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19104)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Assuming you have already trained a logistic regression model named 'model'\n",
    "    # and 'X_train' is your feature matrix\n",
    "\n",
    "    # Get the coefficients (weights) of the logistic regression model\n",
    "    coefficients = model.coef_[0]\n",
    "\n",
    "    # Get the names of the features\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame to store the coefficients and feature names\n",
    "    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Weights': coefficients})\n",
    "\n",
    "    # Sort the DataFrame by coefficient magnitude (absolute value) to identify the most predictive features\n",
    "    coefficients_df = coefficients_df.sort_values(by='Weights', ascending=False)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(coefficients_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6490066225165563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        awry       0.66      0.97      0.79       201\n",
      "     winning       0.14      0.01      0.02       101\n",
      "\n",
      "    accuracy                           0.65       302\n",
      "   macro avg       0.40      0.49      0.40       302\n",
      "weighted avg       0.49      0.65      0.53       302\n",
      "\n",
      "                 Feature   Weights\n",
      "1   d_expression_average  2.155936\n",
      "0      d_content_average -1.463885\n",
      "2     oi_content_average -1.547637\n",
      "3  oi_expression_average -1.766148\n"
     ]
    }
   ],
   "source": [
    "run_logistic_regression(avg_data,'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.652317880794702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        awry       0.66      0.98      0.79       201\n",
      "     winning       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.65       302\n",
      "   macro avg       0.33      0.49      0.39       302\n",
      "weighted avg       0.44      0.65      0.53       302\n",
      "\n",
      "         Feature   Weights\n",
      "1   d_expression  1.277079\n",
      "2     oi_content -1.014712\n",
      "3  oi_expression -1.149079\n",
      "0      d_content -1.226290\n"
     ]
    }
   ],
   "source": [
    "run_logistic_regression(original_data,'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_numeric_labels(text):\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Initialize the result variable\n",
    "    result = 0\n",
    "    \n",
    "    # Check if \"yes\" is present in the text\n",
    "    if 'winning' in text_lower:\n",
    "        result = 1\n",
    "    elif 'awry' in text_lower:\n",
    "        result = 0\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8j/rzq1_zj938vgp1cqknfrmc0m0000gn/T/ipykernel_34549/1427585586.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  avg_data['dataset_numeric'] = avg_data['dataset'].apply(get_dataset_numeric_labels)\n"
     ]
    }
   ],
   "source": [
    "#convert the dataset labels to numbers. winning = 1, awry = 0\n",
    "avg_data['dataset_numeric'] = avg_data['dataset'].apply(get_dataset_numeric_labels)\n",
    "data['dataset_numeric'] = data['dataset'].apply(get_dataset_numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONV_ID', 'id', 'rating_directness_content',\n",
       "       'rating_directness_expression', 'rating_OI_content',\n",
       "       'rating_OI_expression', 'rater_id', 'status', 'last_updated_time',\n",
       "       'dataset', 'd_content', 'd_expression', 'oi_content', 'oi_expression',\n",
       "       'd_content_average', 'd_expression_average', 'oi_content_average',\n",
       "       'oi_expression_average', 'dataset_numeric'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)   # Input size: 4, Output size: 64\n",
    "        self.fc2 = nn.Linear(64, 32)  # Input size: 64, Output size: 32\n",
    "        self.fc3 = nn.Linear(32, 1)   # Input size: 32, Output size: 1\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def neural_net(df):\n",
    "\n",
    "    # Select features and target variable\n",
    "    X = df[['d_content_average', 'd_expression_average', 'oi_content_average', 'oi_expression_average']]\n",
    "    y = df['dataset_numeric']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19104)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Reshape to (batch_size, 1)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # Define the loss function and optimizer - Most popularly used\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)  # AdamW optimizer with weight decay\n",
    "\n",
    "    #TensorDataset: This class is used to wrap tensors representing the input features and target labels into a single dataset object. Each sample in the dataset corresponds to a pair of input features and target labels.\n",
    "    #DataLoader: This class is used to create an iterable over the dataset, enabling you to iterate through batches of data during training. It allows you to specify parameters such as batch size and whether to shuffle the data between epochs.\n",
    "    \n",
    "    # Convert data to DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training the model\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(X_test_tensor)\n",
    "        predictions = (outputs >= 0.5).float()  # Thresholding at 0.5\n",
    "        \n",
    "        # Convert PyTorch tensors to numpy arrays with float32 data type\n",
    "        predictions_np = predictions.numpy().astype('float32')\n",
    "        y_test_np = y_test_tensor.numpy().astype('float32')\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score for each label\n",
    "        precision_per_label = precision_score(y_test_np, predictions_np, average=None)\n",
    "        recall_per_label = recall_score(y_test_np, predictions_np, average=None)\n",
    "        f1_per_label = f1_score(y_test_np, predictions_np, average=None)\n",
    "        \n",
    "        # Print precision, recall, and F1 score for each label\n",
    "        for i in range(len(precision_per_label)):\n",
    "            print(f'Label {i}: Precision: {precision_per_label[i]:.4f}, Recall: {recall_per_label[i]:.4f}, F1 Score: {f1_per_label[i]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision: 0.6656, Recall: 1.0000, F1 Score: 0.7992\n",
      "Label 1: Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "neural_net(avg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def preprocess_for_attention(df):\n",
    "    # Example DataFrame creation (replace this with your actual DataFrame loading)\n",
    "    np.random.seed(19104)  # For reproducible random results\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = ['d_content_average', 'd_expression_average', 'oi_content_average', 'oi_expression_average']\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    # Group by 'CONV_ID' and prepare sequences and targets\n",
    "    grouped = df.groupby('CONV_ID')\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        seq = group[features].values  # Extract features as sequence\n",
    "        target = group['dataset_numeric'].values[-1]  \n",
    "        sequences.append(torch.tensor(seq, dtype=torch.float))\n",
    "        targets.append(torch.tensor(target, dtype=torch.float))\n",
    "\n",
    "    # Padding sequences to have the same length\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert lists to tensor for targets if necessary\n",
    "    y_train = torch.stack(y_train)\n",
    "    y_test = torch.stack(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_for_attention(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.transpose(1, 2), lstm_out)\n",
    "        output = self.fc(attn_applied.squeeze(1))\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_attention_model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim = 4  # Number of input features\n",
    "    hidden_dim = 64\n",
    "    model = LSTMWithAttention(input_dim, hidden_dim)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "        \n",
    "    # Evaluation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_outputs = model(X_test)\n",
    "\n",
    "        # Convert model outputs to binary predictions\n",
    "        preds = torch.sigmoid(test_outputs.squeeze()) >= 0.5\n",
    "        \n",
    "        # Convert tensors to NumPy arrays for sklearn metrics\n",
    "        predictions_np = preds.numpy().astype('float32')\n",
    "        y_test_np = y_test.numpy().astype('float32')\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each label\n",
    "        precision_per_label = precision_score(y_test_np, predictions_np, average=None)\n",
    "        recall_per_label = recall_score(y_test_np, predictions_np, average=None)\n",
    "        f1_per_label = f1_score(y_test_np, predictions_np, average=None)\n",
    "        \n",
    "        # Print precision, recall, and F1 score for each label\n",
    "        for i in range(len(precision_per_label)):\n",
    "            print(f'Label {i}: Precision: {precision_per_label[i]:.4f}, Recall: {recall_per_label[i]:.4f}, F1 Score: {f1_per_label[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision: 0.5556, Recall: 0.8333, F1 Score: 0.6667\n",
      "Label 1: Precision: 0.6667, Recall: 0.3333, F1 Score: 0.4444\n"
     ]
    }
   ],
   "source": [
    "train_attention_model(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

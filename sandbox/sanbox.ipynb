{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the module you want to import\n",
    "tpm_directory = '/Users/priyadcosta/Documents/GitHub/coefficientofconflict/team-process-map/feature_engine'\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(tpm_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Basic Pre-processing\n",
    "\n",
    "Converting the labels to numbers and averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/priyadcosta/Documents/GitHub/coefficientofconflict/tpm-data-anotation/CONFLICT_CONVO_LABELING_LOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the labels into numeric scores\n",
    "\"\"\"\n",
    "\n",
    "def get_numeric_labels(text):\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Initialize the result variable\n",
    "    result = 0\n",
    "    \n",
    "    # Check if \"yes\" is present in the text\n",
    "    if 'yes' in text_lower:\n",
    "        result = 1\n",
    "    elif 'no' in text_lower:\n",
    "        result = 0\n",
    "    \n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "Convert all the columns to numeric labels\n",
    "\"\"\"\n",
    "def convert_labels(df):\n",
    "    \n",
    "    df['d_content'] = df['rating_directness_content'].apply(get_numeric_labels)\n",
    "    df['d_expression'] = df['rating_directness_expression'].apply(get_numeric_labels)\n",
    "    df['oi_content'] = df['rating_OI_content'].apply(get_numeric_labels)\n",
    "    df['oi_expression'] = df['rating_OI_expression'].apply(get_numeric_labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get the average of the ratings for a single column\n",
    "\"\"\"\n",
    "def get_averages(df,on_column):\n",
    "\n",
    "    # Calculate average ratings\n",
    "    average_ratings = df.groupby(['CONV_ID', 'id'])[on_column].mean().reset_index()\n",
    "\n",
    "    # Merge average ratings with original DataFrame\n",
    "    df = df.merge(average_ratings, on=['CONV_ID', 'id'], how='left', suffixes=('', '_average'))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get the average ratings for all the columns\n",
    "\"\"\"\n",
    "def average_labels(df, columns):\n",
    "    for column in columns:\n",
    "        df = get_averages(df, column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determine the labels for the dataset\n",
    "\"\"\"\n",
    "def get_label(conv_id):\n",
    "    if conv_id.endswith('_A') or conv_id.endswith('_B'):\n",
    "        return 'winning'\n",
    "    else:\n",
    "        return 'awry'\n",
    "\n",
    "\"\"\" \n",
    "Get the dataset which the conversation belongs to awry or winning\n",
    "\"\"\"\n",
    "def dataset_labels(df):\n",
    "    df['dataset'] = df['CONV_ID'].apply(get_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop unncessary columns \n",
    "\"\"\"\n",
    "def drop_cols(df,type):\n",
    "    if type == 'average':\n",
    "        return df[['d_content_average', 'd_expression_average', 'oi_content_average','oi_expression_average', 'dataset']]\n",
    "    else:\n",
    "        return df[['d_content', 'd_expression', 'oi_content','oi_expression','dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop the OP's message for winning conversations\n",
    "\"\"\"\n",
    "def drop_op(df):\n",
    "\n",
    "    # Check if there is any row with 'dataset' column value as 'winning'\n",
    "    if (df['dataset'] == 'winning').any():\n",
    "        # Find the first 'CONV_ID' for which 'dataset' column value is 'winning'\n",
    "        first_winning_conv_id = df[df['dataset'] == 'winning']['CONV_ID'].iloc[0]\n",
    "        \n",
    "        # Drop the row(s) with this 'CONV_ID'\n",
    "        df = df[df['CONV_ID'] != first_winning_conv_id]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset to which the chat belongs\n",
    "dataset_labels(data)\n",
    "\n",
    "#convert the text labels to numeric labels\n",
    "convert_labels(data)\n",
    "\n",
    "#get the average rating for each chat\n",
    "numeric_cols = ['d_content', 'd_expression', 'oi_content', 'oi_expression']\n",
    "data = average_labels(data,numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awry convos 32\n",
      "winning convos 26\n"
     ]
    }
   ],
   "source": [
    "print('awry convos ' + str(data[data['dataset'] == 'awry']['CONV_ID'].nunique()))\n",
    "print('winning convos ' + str(data[data['dataset'] == 'winning']['CONV_ID'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = drop_cols(data,'average')\n",
    "original_data = drop_cols(data,'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_logistic_regression(df,target_column):\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    y = df[target_column]   \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19104)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Assuming you have already trained a logistic regression model named 'model'\n",
    "    # and 'X_train' is your feature matrix\n",
    "\n",
    "    # Get the coefficients (weights) of the logistic regression model\n",
    "    coefficients = model.coef_[0]\n",
    "\n",
    "    # Get the names of the features\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame to store the coefficients and feature names\n",
    "    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Weights': coefficients})\n",
    "\n",
    "    # Sort the DataFrame by coefficient magnitude (absolute value) to identify the most predictive features\n",
    "    coefficients_df = coefficients_df.sort_values(by='Weights', ascending=False)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(coefficients_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6490066225165563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        awry       0.66      0.97      0.79       201\n",
      "     winning       0.14      0.01      0.02       101\n",
      "\n",
      "    accuracy                           0.65       302\n",
      "   macro avg       0.40      0.49      0.40       302\n",
      "weighted avg       0.49      0.65      0.53       302\n",
      "\n",
      "                 Feature   Weights\n",
      "1   d_expression_average  2.155936\n",
      "0      d_content_average -1.463885\n",
      "2     oi_content_average -1.547637\n",
      "3  oi_expression_average -1.766148\n"
     ]
    }
   ],
   "source": [
    "run_logistic_regression(avg_data,'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.652317880794702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        awry       0.66      0.98      0.79       201\n",
      "     winning       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.65       302\n",
      "   macro avg       0.33      0.49      0.39       302\n",
      "weighted avg       0.44      0.65      0.53       302\n",
      "\n",
      "         Feature   Weights\n",
      "1   d_expression  1.277079\n",
      "2     oi_content -1.014712\n",
      "3  oi_expression -1.149079\n",
      "0      d_content -1.226290\n"
     ]
    }
   ],
   "source": [
    "run_logistic_regression(original_data,'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_numeric_labels(text):\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Initialize the result variable\n",
    "    result = 0\n",
    "    \n",
    "    # Check if \"yes\" is present in the text\n",
    "    if 'winning' in text_lower:\n",
    "        result = 1\n",
    "    elif 'awry' in text_lower:\n",
    "        result = 0\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8j/rzq1_zj938vgp1cqknfrmc0m0000gn/T/ipykernel_35056/1427585586.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  avg_data['dataset_numeric'] = avg_data['dataset'].apply(get_dataset_numeric_labels)\n"
     ]
    }
   ],
   "source": [
    "#convert the dataset labels to numbers. winning = 1, awry = 0\n",
    "avg_data['dataset_numeric'] = avg_data['dataset'].apply(get_dataset_numeric_labels)\n",
    "data['dataset_numeric'] = data['dataset'].apply(get_dataset_numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CONV_ID', 'id', 'rating_directness_content',\n",
       "       'rating_directness_expression', 'rating_OI_content',\n",
       "       'rating_OI_expression', 'rater_id', 'status', 'last_updated_time',\n",
       "       'dataset', 'd_content', 'd_expression', 'oi_content', 'oi_expression',\n",
       "       'd_content_average', 'd_expression_average', 'oi_content_average',\n",
       "       'oi_expression_average', 'dataset_numeric'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)   # Input size: 4, Output size: 64\n",
    "        self.fc2 = nn.Linear(64, 32)  # Input size: 64, Output size: 32\n",
    "        self.fc3 = nn.Linear(32, 1)   # Input size: 32, Output size: 1\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def neural_net(df):\n",
    "\n",
    "    # Select features and target variable\n",
    "    X = df[['d_content_average', 'd_expression_average', 'oi_content_average', 'oi_expression_average']]\n",
    "    y = df['dataset_numeric']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19104)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Reshape to (batch_size, 1)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # Define the loss function and optimizer - Most popularly used\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)  # AdamW optimizer with weight decay\n",
    "\n",
    "    \"\"\" \n",
    "    TensorDataset: \n",
    "    \n",
    "    This class is used to wrap tensors representing the input features and target labels into a single dataset object. \n",
    "    Each sample in the dataset corresponds to a pair of input features and target labels.\n",
    "    \n",
    "    DataLoader: \n",
    "\n",
    "    This class is used to create an iterable over the dataset, enabling you to iterate through batches of data during training. \n",
    "    It allows you to specify parameters such as batch size and whether to shuffle the data between epochs.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert data to DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training the model\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(X_test_tensor)\n",
    "        predictions = (outputs >= 0.5).float()  # Thresholding at 0.5\n",
    "        \n",
    "        # Convert PyTorch tensors to numpy arrays with float32 data type\n",
    "        predictions_np = predictions.numpy().astype('float32')\n",
    "        y_test_np = y_test_tensor.numpy().astype('float32')\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score for each label\n",
    "        precision_per_label = precision_score(y_test_np, predictions_np, average=None)\n",
    "        recall_per_label = recall_score(y_test_np, predictions_np, average=None)\n",
    "        f1_per_label = f1_score(y_test_np, predictions_np, average=None)\n",
    "        \n",
    "        # Print precision, recall, and F1 score for each label\n",
    "        for i in range(len(precision_per_label)):\n",
    "            print(f'Label {i}: Precision: {precision_per_label[i]:.4f}, Recall: {recall_per_label[i]:.4f}, F1 Score: {f1_per_label[i]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision: 0.6656, Recall: 1.0000, F1 Score: 0.7992\n",
      "Label 1: Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "neural_net(avg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def preprocess_for_attention(df):\n",
    "    # Example DataFrame creation (replace this with your actual DataFrame loading)\n",
    "    np.random.seed(19104)  # For reproducible random results\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = ['d_content_average', 'd_expression_average', 'oi_content_average', 'oi_expression_average']\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    \"\"\"\n",
    "    Grouping by CONV_ID:\n",
    "\n",
    "    This line groups the DataFrame df by the column CONV_ID. \n",
    "    Each group corresponds to a unique conversation identified by CONV_ID. \n",
    "    The purpose is to treat each conversation as a sequence, which is particularly useful for sequence modeling tasks where the context of the conversation is important. \n",
    "    \"\"\"\n",
    "    grouped = df.groupby('CONV_ID')\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare Sequences and Targets:\n",
    "\n",
    "    Iterates over each group created by the groupby operation.\n",
    "\n",
    "    seq = group[features].values extracts just the values of the specified features \n",
    "    (features is a list of column names) from each group as a NumPy array. \n",
    "    This array represents the sequence of observations for a single conversation.\n",
    "\n",
    "    target = group['dataset_numeric'].values[0] extracts the target variable for the sequence. \n",
    "    This example takes the last value of the dataset_numeric column from the group as the target. \n",
    "    The assumption here might be that the target of the entire sequence (conversation) is determined by its final state or message. \n",
    "    \"\"\"\n",
    "\n",
    "    for _, group in grouped:\n",
    "        seq = group[features].values  # Extract features as sequence\n",
    "        target = group['dataset_numeric'].values[0]  # Extract the target variable for the sequence. All the values are same for a given CONV_ID i.e the 0 for awry or 1 for winning\n",
    "        sequences.append(torch.tensor(seq, dtype=torch.float))\n",
    "        targets.append(torch.tensor(target, dtype=torch.float))\n",
    "\n",
    "    \"\"\"\n",
    "    Padding Sequences:\n",
    "\n",
    "    Since the sequences (conversations) can have varying lengths (i.e., different numbers of messages or observations),\n",
    "    they need to be padded to have the same length to be processed in batches by the model. \n",
    "    \n",
    "    The pad_sequence function from PyTorch's torch.nn.utils.rnn module achieves this by adding zeros to shorter sequences until all sequences in the batch have the same length.\n",
    "\n",
    "    The parameter batch_first=True indicates that the output tensor should have a batch size as its first dimension, i.e.,\n",
    "    the tensor shape will be (batch_size, seq_length, features), which is the format expected by most PyTorch models for batched sequence data \n",
    "    \"\"\"\n",
    "\n",
    "    # Padding sequences to have the same length\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, targets, test_size=0.2, random_state=19104)\n",
    "\n",
    "    # Convert lists to tensor for targets if necessary\n",
    "    y_train = torch.stack(y_train)\n",
    "    y_test = torch.stack(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_for_attention(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\" \n",
    "        The input sequence x is passed through the LSTM layer. lstm_out contains the LSTM's output for each time step.\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x) \n",
    "\n",
    "        \"\"\" \n",
    "        The LSTM output is then passed through the attention layer. \n",
    "        This layer assigns a weight to each time step of the LSTM output.\n",
    "        The softmax function ensures that these weights sum up to 1, making them a valid probability distribution.\n",
    "        \"\"\"\n",
    "        attn_weights = F.softmax(self.attention(lstm_out), dim=1) \n",
    "\n",
    "        \"\"\" \n",
    "        The attention weights are then used to compute a weighted sum of the LSTM outputs, which is a way to focus on the most relevant parts of the input sequence. \n",
    "        The function torch.bmm performs a batch matrix-matrix product of the attention weights and LSTM outputs.\n",
    "        \"\"\"\n",
    "        attn_applied = torch.bmm(attn_weights.transpose(1, 2), lstm_out)\n",
    "\n",
    "        \"\"\" \n",
    "        The attention-weighted sum is passed through the final linear layer to produce the model's output.\n",
    "        \"\"\"\n",
    "        output = self.fc(attn_applied.squeeze(1))\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "def train_attention_model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim = 4  # Number of input features\n",
    "    hidden_dim = 64 # Just a Random Number\n",
    "    model = LSTMWithAttention(input_dim, hidden_dim)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs,weights = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_outputs,attn_weights = model(X_test)\n",
    "\n",
    "        # Convert model outputs to binary predictions\n",
    "        preds = torch.sigmoid(test_outputs.squeeze()) >= 0.5\n",
    "        \n",
    "        # Convert tensors to NumPy arrays for sklearn metrics\n",
    "        predictions_np = preds.numpy().astype('float32')\n",
    "        y_test_np = y_test.numpy().astype('float32')\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each label\n",
    "        precision_per_label = precision_score(y_test_np, predictions_np, average=None)\n",
    "        recall_per_label = recall_score(y_test_np, predictions_np, average=None)\n",
    "        f1_per_label = f1_score(y_test_np, predictions_np, average=None)\n",
    "        \n",
    "        # Print precision, recall, and F1 score for each label\n",
    "        for i in range(len(precision_per_label)):\n",
    "            print(f'Label {i}: Precision: {precision_per_label[i]:.4f}, Recall: {recall_per_label[i]:.4f}, F1 Score: {f1_per_label[i]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = train_attention_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution per Time Step: \n",
    "The IG method calculates the contribution of each feature at each time step towards the model's prediction. This enables to see not just which features are important, but also when they are important within the sequence.\n",
    "\n",
    "A time step refers to one point in time in the input sequence. In the context of LSTM models, it's one cycle of processing by the LSTM unit. Is 120 LSTM default timestamp!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop OP\n",
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_specific_conv_id(df, test_conv_id):\n",
    "    np.random.seed(19104)  # For reproducible results\n",
    "    scaler = StandardScaler()\n",
    "    features = ['d_content_average', 'd_expression_average', 'oi_content_average', 'oi_expression_average']\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    \n",
    "    grouped = df.groupby('CONV_ID')\n",
    "    train_sequences = []\n",
    "    train_targets = []\n",
    "    test_sequences = []\n",
    "    test_targets = []\n",
    "\n",
    "    for conv_id, group in grouped:\n",
    "        seq = group[features].values\n",
    "        target = group['dataset_numeric'].values[0]\n",
    "        if conv_id == test_conv_id:\n",
    "            test_sequences.append(torch.tensor(seq, dtype=torch.float))\n",
    "            test_targets.append(torch.tensor(target, dtype=torch.float))\n",
    "        else:\n",
    "            train_sequences.append(torch.tensor(seq, dtype=torch.float))\n",
    "            train_targets.append(torch.tensor(target, dtype=torch.float))\n",
    "    \n",
    "    # Padding sequences\n",
    "    padded_train_sequences = pad_sequence(train_sequences, batch_first=True)\n",
    "    padded_test_sequences = pad_sequence(test_sequences, batch_first=True)\n",
    "\n",
    "    # Converting lists to tensors for targets\n",
    "    y_train = torch.stack(train_targets)\n",
    "    y_test = torch.stack(test_targets)\n",
    "    \n",
    "    return padded_train_sequences, padded_test_sequences, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention_model_iterative(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim = 4  # Number of input features\n",
    "    hidden_dim = 64 # Just a Random Number\n",
    "    model = LSTMWithAttention(input_dim, hidden_dim)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs,weights = model(X_train)\n",
    "        loss = criterion(outputs.squeeze(), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_outputs,attn_weights = model(X_test)\n",
    "\n",
    "        # Convert model outputs to binary predictions\n",
    "        preds = torch.sigmoid(test_outputs.squeeze()) >= 0.5\n",
    "        \n",
    "        # Convert tensors to NumPy arrays for sklearn metrics\n",
    "        predictions_np = preds.numpy().astype('float32')\n",
    "        y_test_np = y_test.numpy().astype('float32')\n",
    "        attn_weights_np = attn_weights.numpy().astype('float32')\n",
    "\n",
    "    return predictions_np,y_test_np,attn_weights_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_testing(df):\n",
    "    unique_conv_ids = df['CONV_ID'].unique()\n",
    "    \n",
    "    predictions_np = []\n",
    "    y_test_np = []\n",
    "    weights_np = []\n",
    "\n",
    "    for test_conv_id in unique_conv_ids:\n",
    "        X_train, X_test, y_train, y_test = preprocess_for_specific_conv_id(df, test_conv_id)\n",
    "        \n",
    "        #train the model\n",
    "        predictions_results, y_test_results,attn_weights_results = train_attention_model_iterative(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        #append and get the average\n",
    "        predictions_np.append(predictions_results)\n",
    "        y_test_np.append(y_test_results)\n",
    "        weights_np.append(attn_weights_results)\n",
    "    \n",
    "     # Convert lists to numpy arrays for efficient numerical operations\n",
    "    predictions_np_array = np.array(predictions_np)\n",
    "    y_test_np_array = np.array(y_test_np)\n",
    "\n",
    "    return predictions_np_array,y_test_np_array,weights_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(predictions_np_array,y_test_np_array,weights_np):\n",
    "    \n",
    "    average_predictions = np.stack(predictions_np_array)\n",
    "    average_y_test = np.stack(y_test_np_array)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each label\n",
    "    precision_per_label = precision_score(average_y_test, average_predictions, average=None)\n",
    "    recall_per_label = recall_score(average_y_test, average_predictions, average=None)\n",
    "    f1_per_label = f1_score(average_y_test, average_predictions, average=None)\n",
    "    \n",
    "    # Print precision, recall, and F1 score for each label\n",
    "    for i in range(len(precision_per_label)):\n",
    "        print(f'Label {i}: Precision: {precision_per_label[i]:.4f}, Recall: {recall_per_label[i]:.4f}, F1 Score: {f1_per_label[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_np_array,y_test_np_array,attributions_np_array = iterative_testing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision: 0.6667, Recall: 0.5625, F1 Score: 0.6102\n",
      "Label 1: Precision: 0.5484, Recall: 0.6538, F1 Score: 0.5965\n"
     ]
    }
   ],
   "source": [
    "plot_results(predictions_np_array,y_test_np_array,attributions_np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label 0: Precision: 0.6538, Recall: 0.5312, F1 Score: 0.5862\n",
    "#### Label 1: Precision: 0.5312, Recall: 0.6538, F1 Score: 0.5862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of neural networks, specifically when using `torch.nn.LSTM` (Long Short-Term Memory) layers from PyTorch, \"timestamps\" refer to the individual time steps in a sequence of data that the LSTM processes. LSTMs are a type of recurrent neural network (RNN) that are particularly good at capturing temporal dynamics and dependencies in sequential data, making them ideal for tasks like time series prediction, natural language processing, and sequence generation.\n",
    "\n",
    "A sequence is a collection of data points ordered in time. Each data point in the sequence is associated with a time step (or \"timestamp\"), which represents a specific point in the sequence's temporal order. In the case of LSTMs, the network processes these sequences one time step at a time, allowing it to maintain a memory of previous inputs through its internal state, which influences the processing of future inputs.\n",
    "\n",
    "When you use `torch.nn.LSTM` in PyTorch, you typically do not explicitly provide \"timestamps\" in the sense of date or time values. Instead, you structure your input data as sequences where the order of the data points represents the temporal order. The LSTM then processes these sequences one element at a time, implicitly understanding the order as the \"timestamps\".\n",
    "\n",
    "The input to an LSTM layer is usually a 3D tensor with dimensions defined as follows:\n",
    "\n",
    "1. **Sequence Length**: The length of the sequence (i.e., the number of timestamps).\n",
    "2. **Batch Size**: The number of sequences processed in parallel during training.\n",
    "3. **Features**: The number of features (or dimensions) in each time step of the sequence.\n",
    "\n",
    "In summary, \"timestamps\" in the context of LSTMs refer to the ordered positions of data points in a sequence that the network processes. The actual representation of time (such as dates or hours) is abstracted away; what matters is the sequential order of the data points, which allows the LSTM to model temporal dependencies and patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
